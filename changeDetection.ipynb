{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change detection framework (CDF)\n",
    "\n",
    "This script is used for training and inference an anomaly detection in time series of EDFAs pump current."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules\n",
    "\n",
    "We will use pandas and numpy for data handling, scikit-learn for scaling and the buildin PCA and the custom clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from entropyAnlysis import EA\n",
    "from fuzzyCMeans import FCM\n",
    "from probabilisticClustering import ProbC\n",
    "from possibilisticClustering import PossC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makros\n",
    "\n",
    "We will need to define certain thresholds and parameters for the algorithms and the stop criteriums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_ENTROPY = 2\n",
    "THRESHOLD_PCA = 0.99\n",
    "\n",
    "TRAINLOOP_CLUSTER = 2\n",
    "TRAINLOOP_EPS = 1E-4\n",
    "TRAINLOOP_MAXEPOCHS = 1\n",
    "\n",
    "ALGOHRITHM_BETA = 2\n",
    "ALGOHRITHM_ETA = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/X_train.xlsx\", header=0)\n",
    "data = np.array(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ea = EA(THRESHOLD_ENTROPY)\n",
    "\n",
    "df_reduced = ea.run(data, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = PCA(THRESHOLD_PCA)\n",
    "\n",
    "principalComponents = pca_data.fit_transform(np.array(df_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(principalComponents)\n",
    "\n",
    "principalComponents = StandardScaler().fit_transform(\n",
    "    principalComponents\n",
    ")\n",
    "\n",
    "X = principalComponents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Fuzzy C-Means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcm = FCM(\n",
    "    data_shape=X.shape,\n",
    "    trainloop_cluster=TRAINLOOP_CLUSTER,\n",
    "    trainloop_maxEpochs=TRAINLOOP_MAXEPOCHS,\n",
    "    trainloop_eps=TRAINLOOP_EPS\n",
    ")\n",
    "\n",
    "fcm.train(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the probabilistic clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probC = ProbC(\n",
    "    data_shape=X.shape,\n",
    "    trainloop_cluster=TRAINLOOP_CLUSTER,\n",
    "    trainloop_maxEpochs=TRAINLOOP_MAXEPOCHS,\n",
    "    trainloop_eps=TRAINLOOP_EPS,\n",
    "    algorithm_beta=ALGOHRITHM_BETA,\n",
    "    algorithm_eta=ALGOHRITHM_ETA\n",
    ")\n",
    "\n",
    "probC.train(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the possibilistic clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possC = PossC(\n",
    "    data_shape=X.shape,\n",
    "    trainloop_cluster=TRAINLOOP_CLUSTER,\n",
    "    trainloop_maxEpochs=TRAINLOOP_MAXEPOCHS,\n",
    "    trainloop_eps=TRAINLOOP_EPS,\n",
    "    algorithm_beta=ALGOHRITHM_BETA,\n",
    "    algorithm_eta=ALGOHRITHM_ETA\n",
    ")\n",
    "\n",
    "possC.train(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(\"data/X_test.xlsx\", header=0)\n",
    "\n",
    "data_test = np.array(df_test)\n",
    "\n",
    "df_test_reduced = ea.invoke(df_test)\n",
    "\n",
    "principalComponents_test = pca_data.transform(np.array(df_test_reduced))\n",
    "\n",
    "principalComponents_test = StandardScaler().fit_transform(principalComponents_test)\n",
    "\n",
    "X_test = principalComponents_test\n",
    "\n",
    "fcm_test = fcm.invoke(X_test)\n",
    "\n",
    "probC_test = probC.invoke(X_test)\n",
    "\n",
    "possC_test = possC.invoke(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
